import os
from typing import Any, Dict
from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import ToolMessage
from langchain.tools import tool
from langchain_pinecone import PineconeVectorStore
from langchain_ollama import OllamaEmbeddings
import json
from langchain_core.documents import Document


load_dotenv()

embeddings = OllamaEmbeddings(model="nomic-embed-text")

vectorStore = PineconeVectorStore(index_name='langchain-collection-768', embedding=embeddings)

model = init_chat_model('llama3.1:latest', model_provider='ollama')

@tool(response_format='content_and_artifact')
def retrive_data(query:str):
        """Retrieve relevant documentation to help answer user queries about Narendra Modi."""

        retrived_docs = vectorStore.as_retriever().invoke(query, k=4)

        # Serialize documents for the model
        serialized = "\n\n".join(
            (f"Source: {doc.metadata.get('source', 'Unknown')}\n\nContent: {doc.page_content}")
            for doc in retrived_docs
        )

        return serialized, retrived_docs

def serialize_document(doc: Document):
    return {
        "id": getattr(doc, "id", None),
        "metadata": getattr(doc, "metadata", {}),
        "page_content": getattr(doc, "page_content", "")
    }

def run_llm(query:str) -> Dict[str,Any]:
    """
    Run the RAG pipeline to answer a query using retrieved documentation.
    
    Args:
        query: The user's question
        
    Returns:
        Dictionary containing:
            - answer: The generated answer
            - context: List of retrieved documents
    """

    # Create the agent with retrieval tool
    system_prompt = (
        "You are a helpful AI assistant that answers questions about Narendra Modi "
        "You have access to a tool that retrieves relevant documentation. "
        "Use the tool to find relevant information before answering questions. "
        "Always cite the sources you use in your answers. "
        "If you cannot find the answer in the retrieved documentation, say so."
    )

    agent = create_agent(model, tools=[retrive_data], system_prompt=system_prompt)

    messages = [{"role": "user", "content": query}]

    response = agent.invoke({"messages": messages})

    answer = response["messages"][-1].content

    context_docs = []

    for message in response["messages"]:
          if isinstance(message, ToolMessage) and hasattr(message, 'artifact'):
                if isinstance(message.artifact, list):
                      context_docs.extend(message.artifact)


    return {
          'answer': answer,
          'context': [serialize_document(doc) for doc in context_docs]
    }


if __name__ == '__main__':
      result = run_llm(query='lalalalal')
      json_str = json.dumps(result, indent=2)
      print('json_str ==>', json_str)